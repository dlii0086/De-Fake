{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-Fake Inference Notebook\n",
    "\n",
    "This notebook demonstrates the inference process for detecting fake images using a combination of CLIP and BLIP models with a neural network classifier.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The de-fake pipeline works as follows:\n",
    "1. **Image Preprocessing**: Load and preprocess the input image\n",
    "2. **Caption Generation**: Use BLIP model to generate a descriptive caption\n",
    "3. **Feature Extraction**: Extract image and text features using CLIP\n",
    "4. **Classification**: Use a neural network to classify the image as real or fake\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "For Google Colab usage:\n",
    "1. This notebook will automatically clone the De-Fake repository\n",
    "2. Install all required dependencies\n",
    "3. Set up the environment\n",
    "\n",
    "Make sure you have the required models downloaded:\n",
    "- `finetune_clip.pt`: Fine-tuned CLIP model\n",
    "- `clip_linear.pt`: Neural network classifier\n",
    "\n",
    "**Note for Colab**: Upload these model files to the Colab environment or place them in the De-Fake directory after cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers clip pillow matplotlib scikit-learn tqdm\n",
    "\n",
    "# Clone the De-Fake repository for BLIP models\n",
    "!git clone https://github.com/dlii0086/De-Fake.git\n",
    "%cd De-Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from blipmodels import blip_decoder\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models\n",
    "\n",
    "Load the pre-trained CLIP, BLIP, and classifier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load BLIP model for caption generation\n",
    "print(\"Loading BLIP model...\")\n",
    "blip_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
    "blip = blip_decoder(pretrained=blip_url, image_size=224, vit='base')\n",
    "blip.eval()\n",
    "blip = blip.to(device)\n",
    "\n",
    "# Define the neural network classifier\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_list, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_list[0])\n",
    "        self.fc2 = nn.Linear(hidden_size_list[0], hidden_size_list[1])\n",
    "        self.fc3 = nn.Linear(hidden_size_list[1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Load the fine-tuned CLIP model and classifier\n",
    "print(\"Loading fine-tuned models...\")\n",
    "try:\n",
    "    model = torch.load(\"finetune_clip.pt\", map_location=device)\n",
    "    linear = NeuralNet(1024, [512, 256], 2).to(device)\n",
    "    linear = torch.load('clip_linear.pt', map_location=device)\n",
    "    print(\"Models loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Model file not found: {e}\")\n",
    "    print(\"Please make sure finetune_clip.pt and clip_linear.pt are in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Define utility functions for image preprocessing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, image_size=224):\n",
    "    \"\"\"Preprocess an image for CLIP model\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((image_size, image_size))\n",
    "    return preprocess(img)\n",
    "\n",
    "def display_image(img_path, title=\"Input Image\"):\n",
    "    \"\"\"Display an image with matplotlib\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    Perform inference on a single image\n",
    "    Returns: prediction (0=real, 1=fake), confidence scores, and generated caption\n",
    "    \"\"\"\n",
    "    # Display the input image\n",
    "    display_image(image_path, \"Input Image for Analysis\")\n",
    "    \n",
    "    # Preprocess image for BLIP\n",
    "    tform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = tform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption with BLIP\n",
    "    print(\"Generating caption with BLIP...\")\n",
    "    with torch.no_grad():\n",
    "        caption = blip.generate(img_tensor, sample=False, num_beams=3, max_length=60, min_length=5)\n",
    "    \n",
    "    generated_caption = caption[0] if isinstance(caption, list) else caption\n",
    "    print(f\"Generated caption: {generated_caption}\")\n",
    "    \n",
    "    # Preprocess for CLIP\n",
    "    image = preprocess_image(image_path).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([generated_caption]).to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features with CLIP...\")\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "        # Concatenate features\n",
    "        emb = torch.cat((image_features, text_features), 1)\n",
    "        \n",
    "        # Classify\n",
    "        output = linear(emb.float())\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predict = output.argmax(1)\n",
    "        \n",
    "    return predict.cpu().numpy()[0], probabilities.cpu().numpy()[0], generated_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Inference\n",
    "\n",
    "Run inference on an image and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = \"CLIP.png\"  # Replace with your image path\n",
    "\n",
    "# Check if the file exists\n",
    "if not Path(image_path).exists():\n",
    "    print(f\"Image file {image_path} not found. Please provide a valid image path.\")\n",
    "    print(\"Available files in current directory:\")\n",
    "    for file in Path('.').glob('*'):\n",
    "        if file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "            print(f\"  - {file.name}\")\n",
    "else:\n",
    "    # Run inference\n",
    "    prediction, confidence, caption = predict_image(image_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INFERENCE RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "    print(f\"Prediction: {'FAKE' if prediction == 1 else 'REAL'}\")\n",
    "    print(f\"Confidence Scores: Real={confidence[0]:.4f}, Fake={confidence[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing\n",
    "\n",
    "Process multiple images in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(image_dir):\n",
    "    \"\"\"Process all images in a directory\"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    if not image_dir.exists():\n",
    "        print(f\"Directory {image_dir} not found.\")\n",
    "        return\n",
    "    \n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:\n",
    "        image_files.extend(list(image_dir.glob(ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process.\")\n",
    "    \n",
    "    results = []\n",
    "    for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "        try:\n",
    "            prediction, confidence, caption = predict_image(str(img_path))\n",
    "            results.append({\n",
    "                'image_path': str(img_path),\n",
    "                'prediction': 'FAKE' if prediction == 1 else 'REAL',\n",
    "                'real_confidence': float(confidence[0]),\n",
    "                'fake_confidence': float(confidence[1]),\n",
    "                'caption': caption\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example batch processing\n",
    "# results = process_directory(\"path/to/your/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Details\n",
    "\n",
    "Understanding the model architecture used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "print(\"Neural Network Classifier Architecture:\")\n",
    "print(linear)\n",
    "\n",
    "print(\"\\nInput size: 1024 (512 image features + 512 text features)\")\n",
    "print(\"Hidden layers: [512, 256]\")\n",
    "print(\"Output size: 2 (Real vs Fake)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Metrics\n",
    "\n",
    "Evaluate model performance on test data (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_images, test_labels):\n",
    "    \"\"\"Evaluate model performance on test data\"\"\"\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        prediction, confidence, _ = predict_image(img_path)\n",
    "        predictions.append(prediction)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "# Example usage (requires test data)\n",
    "# test_images = [\"path1.jpg\", \"path2.jpg\", ...]\n",
    "# test_labels = [0, 1, ...]  # 0=real, 1=fake\n",
    "# predictions, confidences = evaluate_model(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps & Colab Instructions\n",
    "\n",
    "### For Google Colab Usage:\n",
    "\n",
    "1. **Upload this notebook to Google Colab**\n",
    "2. **Run all cells sequentially** - The setup cell will automatically:\n",
    "   - Install required packages\n",
    "   - Clone the De-Fake repository\n",
    "   - Change to the De-Fake directory\n",
    "3. **Upload model files**: After running the setup cell, upload `finetune_clip.pt` and `clip_linear.pt` to the Colab environment\n",
    "4. **Upload test images**: Place your images in the Colab file system or use Google Drive integration\n",
    "\n",
    "### For Local Usage:\n",
    "\n",
    "1. **Ensure models are available**: Place `finetune_clip.pt` and `clip_linear.pt` in the current directory\n",
    "2. **Prepare your images**: Place images in a directory or use individual file paths\n",
    "3. **Run cells sequentially**: Execute cells in order from top to bottom\n",
    "4. **Customize for your needs**: Modify the image paths and parameters as needed\n",
    "\n",
    "This notebook provides a complete inference pipeline for the De-Fake project, replicating the functionality of `test.py` in an interactive Jupyter environment that works both locally and on Google Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexicon": "python6",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
